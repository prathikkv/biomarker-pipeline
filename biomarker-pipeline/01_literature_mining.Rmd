---
title: "CAMK2D Literature Mining & PubMed Analysis"
subtitle: "Comprehensive Analysis of CAMK2D Research Literature"
author: "CAMK2D Research Automation System"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    number_sections: true
    theme: flatly
    highlight: tango
    code_folding: show
    df_print: paged
  pdf_document:
    toc: true
    number_sections: true
    keep_tex: false
  word_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
# Chunk options for consistent formatting
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  fig.width = 10,
  fig.height = 6,
  fig.align = 'center',
  cache = FALSE,
  comment = "#>"
)

# Set random seed for reproducibility
set.seed(42)
```

# Executive Summary

This document provides a comprehensive analysis of CAMK2D (Calcium/Calmodulin Dependent Protein Kinase II Delta) research literature using automated PubMed mining and text analysis. The analysis focuses on identifying research trends, key concepts, author networks, and knowledge gaps in CAMK2D research, particularly in relation to atrial fibrillation (aFIB) and heart failure (HF).

## Key Findings Summary

- **Publication Timeline**: Analysis of research trends over time
- **Research Focus**: Identification of primary research areas and methodologies  
- **Author Networks**: Collaboration patterns and influential researchers
- **Knowledge Gaps**: Areas requiring further investigation
- **Clinical Relevance**: Connections to cardiac diseases and therapeutic targets

---

# Methods Section

## Package Loading and Configuration

```{r load-packages}
# Essential packages for literature mining and analysis
library(tidyverse)    # Data manipulation and visualization
library(rentrez)      # Primary NCBI database access
# library(RISmed)   # Not used - using rentrez instead
library(tm)           # Text mining framework
library(wordcloud)    # Word cloud generation
library(wordcloud2)   # Enhanced word clouds
library(topicmodels)  # Topic modeling
library(igraph)       # Network analysis
library(visNetwork)   # Interactive network visualization
library(plotly)       # Interactive plots
library(DT)           # Interactive tables
library(openxlsx)     # Excel export
library(kableExtra)   # Enhanced table formatting
library(lubridate)    # Date handling
library(stringr)      # String manipulation
library(tidytext)     # Text mining with tidy data
library(SnowballC)    # Text stemming

# Check package versions for reproducibility
cat("Package versions:\n")
#cat("RISmed:", packageVersion("RISmed"), "\n")
#cat("rentrez:", packageVersion("rentrez"), "\n")
#cat("tm:", packageVersion("tm"), "\n")
#cat("tidyverse:", packageVersion("tidyverse"), "\n")
```

## Search Strategy Design

### Why RISmed is Chosen
RISmed provides direct access to the PubMed database through R, allowing automated querying and data extraction. It handles XML parsing, rate limiting, and provides structured access to publication metadata.

```{r search-strategy}
# Define comprehensive search terms
camk_terms <- c("CAMK2D", "CAMK2A", "CAMK2B", "CAMK2G", 
                "calcium calmodulin dependent kinase",
                "CaMKII", "CaMK2", "CaM kinase II")

cardiac_terms <- c("atrial fibrillation", "heart failure", "cardiac",
                   "cardiomyopathy", "arrhythmia", "myocardial",
                   "cardiovascular", "heart", "atrial", "ventricular")

additional_terms <- c("phosphorylation", "kinase", "calcium signaling",
                     "cardiac electrophysiology", "ion channels")

# Create search query combinations
create_search_query <- function(camk_terms, cardiac_terms, additional_terms = NULL) {
  # Combine CAMK terms with OR
  camk_query <- paste0("(", paste(camk_terms, collapse = " OR "), ")")
  
  # Combine cardiac terms with OR  
  cardiac_query <- paste0("(", paste(cardiac_terms, collapse = " OR "), ")")
  
  # Main query: CAMK AND cardiac terms
  main_query <- paste(camk_query, "AND", cardiac_query)
  
  # Add additional terms if provided
  if (!is.null(additional_terms)) {
    additional_query <- paste0("(", paste(additional_terms, collapse = " OR "), ")")
    main_query <- paste(main_query, "AND", additional_query)
  }
  
  return(main_query)
}

# Generate multiple search strategies
search_queries <- list(
  broad = create_search_query(camk_terms, cardiac_terms),
  specific = create_search_query(camk_terms[1:4], cardiac_terms[1:3]),
  phosphorylation = create_search_query(camk_terms, cardiac_terms, "phosphorylation"),
  electrophysiology = create_search_query(camk_terms, cardiac_terms, "electrophysiology")
)

# Display search strategies
cat("Search Strategies:\n")
for (i in 1:length(search_queries)) {
  cat(names(search_queries)[i], ":\n", search_queries[[i]], "\n\n")
}
```

## Data Extraction Functions

```{r extraction-functions}
# Function to perform PubMed search using rentrez (more reliable)
perform_pubmed_search <- function(query, max_results = 1000, start_year = 2000) {
  cat("Searching PubMed with query:", query, "\n")
  cat("Maximum results:", max_results, "\n")
  cat("Start year:", start_year, "\n\n")
  
  tryCatch({
    # Create date range for search
    current_year <- year(Sys.Date())
    date_range <- paste0(start_year, ":", current_year, "[pdat]")
    full_query <- paste(query, "AND", date_range)
    
    # Perform the search using rentrez
    search_results <- entrez_search(
      db = "pubmed",
      term = full_query,
      retmax = max_results
    )
    
    cat("Found", length(search_results$ids), "publications\n")
    
    # Return the search results with IDs
    if (length(search_results$ids) > 0) {
      return(search_results)
    } else {
      return(NULL)
    }
    
  }, error = function(e) {
    cat("Error in PubMed search:", e$message, "\n")
    # Try a simpler query if the complex one fails
    tryCatch({
      cat("Trying simplified search...\n")
      simple_results <- entrez_search(
        db = "pubmed",
        term = query,
        retmax = min(max_results, 100)  # Reduce results for reliability
      )
      
      if (length(simple_results$ids) > 0) {
        cat("Simplified search found", length(simple_results$ids), "publications\n")
        return(simple_results)
      } else {
        return(NULL)
      }
    }, error = function(e2) {
      cat("Both search attempts failed:", e2$message, "\n")
      return(NULL)
    })
  })
}

# Function to extract metadata from PubMed records
extract_metadata <- function(search_results) {
  if (is.null(search_results) || length(search_results$ids) == 0) {
    return(NULL)
  }
  
  tryCatch({
    # Fetch summaries for the articles (more reliable than full records)
    cat("Fetching article summaries for", length(search_results$ids), "publications...\n")
    
    # Fetch all results using batch processing to avoid HTTP 414 errors
    ids_to_fetch <- search_results$ids
    batch_size <- 20  # Process in smaller batches to avoid API limits
    
    cat("Processing", length(ids_to_fetch), "publications in batches of", batch_size, "\n")
    
    # Process in batches
    all_summaries <- list()
    
    for (batch_start in seq(1, length(ids_to_fetch), by = batch_size)) {
      batch_end <- min(batch_start + batch_size - 1, length(ids_to_fetch))
      batch_ids <- ids_to_fetch[batch_start:batch_end]
      
      cat("Processing batch", ceiling(batch_start/batch_size), "of", 
          ceiling(length(ids_to_fetch)/batch_size), "(", length(batch_ids), "publications)\n")
      
      # Add delay to respect NCBI rate limits
      if (batch_start > 1) {
        Sys.sleep(0.5)  # 500ms delay between batches
      }
      
      tryCatch({
        batch_summaries <- entrez_summary(db = "pubmed", id = batch_ids)
        
        # Handle single vs multiple results
        if (length(batch_ids) == 1) {
          batch_summaries <- list(batch_summaries)
        }
        
        # Add to master list
        all_summaries <- c(all_summaries, batch_summaries)
        
      }, error = function(e) {
        cat("Warning: Batch", ceiling(batch_start/batch_size), "failed:", e$message, "\n")
        # Add placeholder entries for failed batch
        for (id in batch_ids) {
          all_summaries <<- c(all_summaries, list(list(uid = id, error = TRUE)))
        }
      })
    }
    
    summaries <- all_summaries
    
    # Extract metadata from summaries
    metadata_list <- list()
    
    for (i in seq_along(summaries)) {
      tryCatch({
        summary_data <- summaries[[i]]
        
        # Check if this is an error placeholder
        if (!is.null(summary_data$error) && summary_data$error) {
          metadata_list[[i]] <- data.frame(
            PMID = as.character(summary_data$uid),
            Title = "Title not available (API error)",
            Authors = "Authors not available",
            Journal = "Journal not available",
            Year = year(Sys.Date()),
            stringsAsFactors = FALSE
          )
          next
        }
        
        # Extract authors safely
        authors <- ""
        if (!is.null(summary_data$authors)) {
          if (is.data.frame(summary_data$authors) && "name" %in% names(summary_data$authors)) {
            # PubMed returns authors as data.frame with 'name' column
            authors <- paste(summary_data$authors$name, collapse = ", ")
          } else if (is.character(summary_data$authors)) {
            # Sometimes authors are returned as character vector
            authors <- paste(summary_data$authors, collapse = ", ")
          } else if (is.list(summary_data$authors)) {
            # Fallback for other list structures
            author_names <- sapply(summary_data$authors, function(x) {
              if (is.character(x)) return(x)
              if (is.list(x) && !is.null(x$name)) return(x$name)
              return("")
            })
            authors <- paste(author_names[author_names != ""], collapse = ", ")
          }
        }
        
        # Extract publication year safely
        pub_year <- NA
        if (!is.null(summary_data$pubdate)) {
          year_match <- regexpr("\\d{4}", summary_data$pubdate)
          if (year_match > 0) {
            pub_year <- as.numeric(substr(summary_data$pubdate, year_match, year_match + 3))
          }
        }
        
        # Create data frame with safe extraction
        metadata_list[[i]] <- data.frame(
          PMID = ifelse(!is.null(summary_data$uid), as.character(summary_data$uid), paste0("unknown_", i)),
          Title = ifelse(!is.null(summary_data$title) && summary_data$title != "", 
                        summary_data$title, "Title not available"),
          Authors = ifelse(authors != "", authors, "Authors not available"),
          Journal = ifelse(!is.null(summary_data$source) && summary_data$source != "", 
                          summary_data$source, "Journal not available"),
          Year = ifelse(!is.na(pub_year), pub_year, year(Sys.Date())),
          stringsAsFactors = FALSE
        )
        
      }, error = function(e) {
        # If extraction fails for this record, create minimal entry
        metadata_list[[i]] <- data.frame(
          PMID = paste0("error_", i),
          Title = "Title extraction failed",
          Authors = "Authors extraction failed",
          Journal = "Journal extraction failed",
          Year = year(Sys.Date()),
          stringsAsFactors = FALSE
        )
      })
    }
    
    # Combine all metadata
    metadata <- do.call(rbind, metadata_list)
    
    cat("Extracted metadata for", nrow(metadata), "publications\n")
    
    # Show preview
    cat("\nData preview:\n")
    print(head(metadata[, c("PMID", "Title", "Journal", "Year")]))
    
    return(metadata)
    
  }, error = function(e) {
    cat("Error extracting metadata:", e$message, "\n")
    
    # Fallback: create minimal dataset with just PMIDs
    cat("Creating minimal dataset with PMIDs only...\n")
    minimal_data <- data.frame(
      PMID = search_results$ids,
      Title = "Title not available",
      Authors = "Authors not available", 
      Journal = "Journal not available",
      Year = year(Sys.Date()),
      stringsAsFactors = FALSE
    )
    
    return(minimal_data)
  })
}
```

---

# Data Collection and Processing

## PubMed Database Search

```{r pubmed-search}
# Perform searches for all strategies
cat("=== Starting Literature Search ===\n\n")

all_records <- list()
all_metadata <- list()

for (strategy in names(search_queries)) {
  cat("Executing search strategy:", strategy, "\n")
  
  # Perform search
  records <- perform_pubmed_search(
    query = search_queries[[strategy]],
    max_results = 500,
    start_year = 2000
  )
  
  # Extract metadata
  metadata <- extract_metadata(records)
  
  if (!is.null(metadata)) {
    all_records[[strategy]] <- records
    all_metadata[[strategy]] <- metadata
    
    cat("Strategy", strategy, "returned", nrow(metadata), "publications\n\n")
  } else {
    cat("Strategy", strategy, "returned no results\n\n")
  }
}

# Combine all metadata
if (length(all_metadata) > 0) {
  combined_metadata <- bind_rows(all_metadata, .id = "SearchStrategy")
  
  # Remove duplicates based on PMID
  combined_metadata <- combined_metadata %>%
    distinct(PMID, .keep_all = TRUE) %>%
    arrange(desc(Year))
  
  cat("Combined dataset contains", nrow(combined_metadata), "unique publications\n")
} else {
  stop("No data retrieved from PubMed searches")
}

# Display summary statistics
cat("\nDataset Summary:\n")
summary(combined_metadata) %>% print()
```

## Data Quality Assessment

```{r data-quality}
# Assess data completeness
cat("=== Data Quality Assessment ===\n\n")

quality_metrics <- combined_metadata %>%
  summarise(
    total_records = n(),
    complete_titles = sum(!is.na(Title) & Title != "" & Title != "Title not available"),
    complete_authors = sum(!is.na(Authors) & Authors != "" & Authors != "Authors not available"),
    complete_journals = sum(!is.na(Journal) & Journal != "" & Journal != "Journal not available"),
    complete_years = sum(!is.na(Year)),
    .groups = 'drop'
  ) %>%
  mutate(
    title_completeness = round(complete_titles / total_records * 100, 1),
    author_completeness = round(complete_authors / total_records * 100, 1),
    journal_completeness = round(complete_journals / total_records * 100, 1),
    year_completeness = round(complete_years / total_records * 100, 1)
  )

# Display quality metrics
quality_metrics %>%
  select(total_records, ends_with("_completeness")) %>%
  kable(
    caption = "Data Completeness Assessment (%)",
    col.names = c("Total Records", "Title", "Authors", "Journal", "Year")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Year distribution
# Ensure Year is numeric (in case it's a list or character)
combined_metadata$Year <- as.numeric(as.character(combined_metadata$Year))

year_dist <- combined_metadata %>%
  filter(!is.na(Year)) %>%
  dplyr::count(Year) %>%
  arrange(Year)

cat("\nPublication year range:", min(year_dist$Year), "-", max(year_dist$Year), "\n")
```

## Text Preprocessing

```{r text-preprocessing}
# Create text corpus for analysis
create_text_corpus <- function(text_data) {
  # Remove NA values
  text_data <- text_data[!is.na(text_data)]
  
  # Create corpus
  corpus <- Corpus(VectorSource(text_data))
  
  # Text preprocessing pipeline
  corpus <- corpus %>%
    tm_map(content_transformer(tolower)) %>%       # Convert to lowercase
    tm_map(removePunctuation) %>%                  # Remove punctuation
    tm_map(removeNumbers) %>%                      # Remove numbers
    tm_map(removeWords, stopwords("english")) %>%  # Remove English stopwords
    tm_map(removeWords, c("camk", "camk2", "camkii", "protein", "kinase", 
                         "expression", "using", "results", "study", "analysis",
                         "method", "conclusion", "background", "objective")) %>%
    tm_map(stripWhitespace) %>%                    # Remove extra whitespace
    tm_map(stemDocument)                           # Stem words
  
  return(corpus)
}

# Process titles and abstracts
cat("Processing text data...\n")

# Combine titles and available text for comprehensive analysis
text_for_analysis <- paste(
  ifelse(is.na(combined_metadata$Title), "", combined_metadata$Title),
  ifelse(is.na(combined_metadata$Journal), "", combined_metadata$Journal),
  sep = " "
)

# Create corpus
text_corpus <- create_text_corpus(text_for_analysis)

# Create document-term matrix
dtm <- DocumentTermMatrix(text_corpus)

cat("Document-term matrix dimensions:", dim(dtm), "\n")

# Remove sparse terms (appear in less than 1% of documents)
dtm_reduced <- removeSparseTerms(dtm, 0.99)
cat("Reduced matrix dimensions:", dim(dtm_reduced), "\n")

# Get term frequencies
term_freq <- colSums(as.matrix(dtm_reduced))
term_freq <- sort(term_freq, decreasing = TRUE)

cat("Top 20 most frequent terms:\n")
head(term_freq, 20) %>% 
  data.frame(Term = names(.), Frequency = .) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

---

# Results and Visualizations

## Publication Timeline Analysis

```{r publication-timeline}
# Publication trends over time
timeline_data <- combined_metadata %>%
  filter(!is.na(Year), Year >= 2000) %>%
  dplyr::count(Year) %>%
  arrange(Year)

# Create timeline plot
timeline_plot <- ggplot(timeline_data, aes(x = Year, y = n)) +
  geom_line(color = "steelblue", size = 1.2) +
  geom_point(color = "darkblue", size = 2) +
  geom_smooth(method = "loess", se = TRUE, alpha = 0.3, color = "red") +
  labs(
    title = "CAMK2D Research Publication Timeline",
    subtitle = "Number of publications per year (2000-present)",
    x = "Publication Year",
    y = "Number of Publications",
    caption = "Data source: PubMed via RISmed"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 12),
    axis.title = element_text(size = 11),
    axis.text = element_text(size = 10)
  ) +
  scale_x_continuous(breaks = seq(2000, 2024, 2))

# Display plot
print(timeline_plot)

# Export high-quality figure
ggsave(filename = "figures/literature_timeline_trends.png", 
       plot = timeline_plot, 
       width = 12, height = 8, dpi = 300)
cat("✅ Figure saved: figures/literature_timeline_trends.png\n")

# Interactive version
timeline_interactive <- ggplotly(timeline_plot, tooltip = c("x", "y"))
timeline_interactive

# Summary statistics
cat("Publication Timeline Summary:\n")
cat("Total publications:", sum(timeline_data$n), "\n")
cat("Years covered:", min(timeline_data$Year), "-", max(timeline_data$Year), "\n")
cat("Average publications per year:", round(mean(timeline_data$n), 1), "\n")
cat("Peak year:", timeline_data$Year[which.max(timeline_data$n)], 
    "with", max(timeline_data$n), "publications\n")
```

## Journal Distribution Analysis

```{r journal-analysis}
# Analyze journal distribution
journal_data <- combined_metadata %>%
  filter(!is.na(Journal), Journal != "") %>%
  dplyr::count(Journal, sort = TRUE) %>%
  slice_head(n = 15)

# Create journal distribution plot
journal_plot <- ggplot(journal_data, aes(x = reorder(Journal, n), y = n)) +
  geom_col(fill = "steelblue", alpha = 0.8) +
  coord_flip() +
  labs(
    title = "Top 15 Journals Publishing CAMK2D Research",
    x = "Journal",
    y = "Number of Publications",
    caption = "Data source: PubMed via RISmed"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 11),
    axis.text.y = element_text(size = 9),
    axis.text.x = element_text(size = 10)
  )

print(journal_plot)

# Export high-quality figure
ggsave(filename = "figures/journal_distribution.png", 
       plot = journal_plot, 
       width = 12, height = 8, dpi = 300)
cat("✅ Figure saved: figures/journal_distribution.png\n")

# Journal distribution table
journal_data %>%
  rename(`Journal Name` = Journal, `Publications` = n) %>%
  kable(caption = "Top 15 Journals by Publication Count") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Impact analysis
cat("Journal Distribution Summary:\n")
cat("Total unique journals:", nrow(combined_metadata %>% distinct(Journal)), "\n")
cat("Top journal:", journal_data$Journal[1], "with", journal_data$n[1], "publications\n")
cat("Top 5 journals account for", 
    round(sum(journal_data$n[1:5]) / sum(journal_data$n) * 100, 1), 
    "% of publications\n")
```

## Word Frequency and Concept Analysis

```{r word-analysis}
# Create word frequency analysis
top_terms <- head(term_freq, 50)

# Word frequency bar plot
word_freq_df <- data.frame(
  Term = names(top_terms),
  Frequency = as.numeric(top_terms),
  stringsAsFactors = FALSE
) %>%
  slice_head(n = 20)

word_plot <- ggplot(word_freq_df, aes(x = reorder(Term, Frequency), y = Frequency)) +
  geom_col(fill = "darkgreen", alpha = 0.8) +
  coord_flip() +
  labs(
    title = "Top 20 Most Frequent Terms in CAMK2D Literature",
    x = "Terms",
    y = "Frequency",
    caption = "After removing common stopwords and CAMK-related terms"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(size = 14, face = "bold"),
    axis.title = element_text(size = 11),
    axis.text = element_text(size = 10)
  )

print(word_plot)

# Export high-quality figure
ggsave(filename = "figures/top_keywords.png", 
       plot = word_plot, 
       width = 12, height = 8, dpi = 300)
cat("✅ Figure saved: figures/top_keywords.png\n")

# Word cloud generation
if (length(term_freq) > 10) {
  # Static word cloud
  wordcloud(
    words = names(term_freq),
    freq = term_freq,
    min.freq = 2,
    max.words = 100,
    random.order = FALSE,
    rot.per = 0.35,
    colors = brewer.pal(8, "Dark2"),
    scale = c(3, 0.5)
  )
  
  # Interactive word cloud
  wordcloud2_plot <- wordcloud2(
    data = data.frame(word = names(head(term_freq, 100)), 
                     freq = head(term_freq, 100)),
    size = 0.8,
    minRotation = -pi/6,
    maxRotation = pi/6,
    rotateRatio = 0.4
  )
  
  print(wordcloud2_plot)
}

# Term frequency table
head(term_freq, 25) %>%
  data.frame(Term = names(.), Frequency = .) %>%
  kable(caption = "Top 25 Most Frequent Terms") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

## Author Network Analysis

```{r author-network}
# Extract and analyze author networks
extract_authors <- function(author_string) {
  if (is.na(author_string) || author_string == "") {
    return(NULL)
  }
  
  # Split authors and clean names
  authors <- strsplit(author_string, ",\\s*")[[1]]
  authors <- trimws(authors)
  authors <- authors[authors != "" & !is.na(authors)]
  
  return(authors)
}

# Create author network data
author_data <- combined_metadata %>%
  filter(!is.na(Authors), Authors != "") %>%
  rowwise() %>%
  mutate(AuthorList = list(extract_authors(Authors))) %>%
  ungroup()

# Find most prolific authors
all_authors <- unlist(lapply(author_data$AuthorList, function(x) x))
author_freq <- table(all_authors)
author_freq <- sort(author_freq, decreasing = TRUE)

# Top authors
top_authors <- head(author_freq, 20)

cat("Top 20 Most Prolific Authors:\n")
top_authors %>%
  data.frame(Author = names(.), Publications = .) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Create collaboration network for top authors
top_author_names <- names(head(author_freq, 15))

# Build collaboration edges
collaboration_edges <- data.frame(
  from = character(),
  to = character(),
  weight = numeric(),
  stringsAsFactors = FALSE
)

for (i in 1:nrow(author_data)) {
  authors <- author_data$AuthorList[[i]]
  if (length(authors) > 1) {
    # Create combinations of collaborating authors
    author_pairs <- combn(authors, 2, simplify = FALSE)
    
    for (pair in author_pairs) {
      if (all(pair %in% top_author_names)) {
        # Add edge
        collaboration_edges <- rbind(
          collaboration_edges,
          data.frame(from = pair[1], to = pair[2], weight = 1)
        )
      }
    }
  }
}

# Aggregate collaboration weights
if (nrow(collaboration_edges) > 0) {
  collaboration_edges <- collaboration_edges %>%
    group_by(from, to) %>%
    summarise(weight = sum(weight), .groups = 'drop')
  
  # Create network graph
  if (nrow(collaboration_edges) > 0) {
    network_graph <- graph_from_data_frame(collaboration_edges, directed = FALSE)
    
    # Calculate network metrics
    degree_values <- degree(network_graph)
    vertex_metrics <- data.frame(
      Author = V(network_graph)$name,
      Degree = as.numeric(unname(degree_values)),
      Betweenness = as.numeric(unname(betweenness(network_graph))),
      Closeness = as.numeric(unname(closeness(network_graph))),
      Publications = as.numeric(author_freq[match(V(network_graph)$name, names(author_freq))])
    )
    
    # Network visualization
    set.seed(42)
    plot(
      network_graph,
      vertex.size = sqrt(vertex_metrics$Publications) * 3,
      vertex.label.cex = 0.8,
      vertex.color = "lightblue",
      edge.width = E(network_graph)$weight * 2,
      edge.color = "gray",
      layout = layout_with_fr(network_graph),
      main = "CAMK2D Research Author Collaboration Network"
    )
    
    cat("Network Summary:\n")
    cat("Nodes (Authors):", vcount(network_graph), "\n")
    cat("Edges (Collaborations):", ecount(network_graph), "\n")
    cat("Average degree:", round(mean(as.numeric(degree_values)), 2), "\n")
  }
}
```

## Topic Modeling Analysis

```{r topic-modeling}
# Perform topic modeling on abstracts
if (nrow(dtm_reduced) > 5 && ncol(dtm_reduced) > 5) {
  
  # Convert DTM to format suitable for topicmodels
  dtm_for_lda <- dtm_reduced[rowSums(as.matrix(dtm_reduced)) > 0, ]
  
  if (nrow(dtm_for_lda) > 5) {
    # Fit LDA model with 5 topics
    lda_model <- LDA(dtm_for_lda, k = 5, control = list(seed = 42))
    
    # Extract topics
    topics <- tidy(lda_model, matrix = "beta") %>%
      group_by(topic) %>%
      slice_max(beta, n = 10) %>%
      ungroup() %>%
      arrange(topic, -beta)
    
    # Visualize topics
    topic_plot <- topics %>%
      mutate(term = reorder_within(term, beta, topic)) %>%
      ggplot(aes(beta, term, fill = factor(topic))) +
      geom_col(show.legend = FALSE) +
      facet_wrap(~ topic, scales = "free") +
      scale_y_reordered() +
      labs(
        title = "Top Terms by Topic in CAMK2D Research",
        x = "Beta (Term Probability)",
        y = "Terms"
      ) +
      theme_minimal()
    
    print(topic_plot)
    
    # Export high-quality figure
    ggsave(filename = "figures/topic_analysis.png", 
           plot = topic_plot, 
           width = 12, height = 8, dpi = 300)
    cat("✅ Figure saved: figures/topic_analysis.png\n")
    
    # Topic interpretation
    cat("Topic Modeling Results:\n")
    for (i in 1:5) {
      topic_terms <- topics %>%
        filter(topic == i) %>%
        slice_head(n = 5) %>%
        pull(term)
      
      cat("Topic", i, ":", paste(topic_terms, collapse = ", "), "\n")
    }
  }
}
```

---

# Discussion and Interpretation

## Research Trends Analysis

```{r trends-analysis}
# Analyze research trends by combining temporal and content analysis
trend_analysis <- combined_metadata %>%
  filter(!is.na(Year), Year >= 2000) %>%
  mutate(
    Period = case_when(
      Year >= 2000 & Year < 2005 ~ "2000-2004",
      Year >= 2005 & Year < 2010 ~ "2005-2009", 
      Year >= 2010 & Year < 2015 ~ "2010-2014",
      Year >= 2015 & Year < 2020 ~ "2015-2019",
      Year >= 2020 ~ "2020-present"
    )
  ) %>%
  dplyr::count(Period) %>%
  mutate(
    Percentage = round(n / sum(n) * 100, 1)
  )

# Display trend analysis
trend_analysis %>%
  kable(
    caption = "Research Distribution by Time Period",
    col.names = c("Time Period", "Publications", "Percentage (%)")
  ) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Research focus areas analysis
focus_analysis <- combined_metadata %>%
  mutate(
    HasCardiac = str_detect(tolower(Title), "cardiac|heart|cardiovascular|atrial"),
    HasPhosphorylation = str_detect(tolower(Title), "phosphoryl"),
    HasArrhythmia = str_detect(tolower(Title), "arrhythmia|fibrillation"),
    HasSignaling = str_detect(tolower(Title), "signal|pathway")
  ) %>%
  summarise(
    Total = n(),
    Cardiac_Focus = sum(HasCardiac, na.rm = TRUE),
    Phosphorylation_Focus = sum(HasPhosphorylation, na.rm = TRUE),
    Arrhythmia_Focus = sum(HasArrhythmia, na.rm = TRUE),
    Signaling_Focus = sum(HasSignaling, na.rm = TRUE)
  ) %>%
  mutate(
    Cardiac_Pct = round(Cardiac_Focus / Total * 100, 1),
    Phosphorylation_Pct = round(Phosphorylation_Focus / Total * 100, 1),
    Arrhythmia_Pct = round(Arrhythmia_Focus / Total * 100, 1),
    Signaling_Pct = round(Signaling_Focus / Total * 100, 1)
  )

cat("Research Focus Areas Analysis:\n")
cat("Cardiac-related research:", focus_analysis$Cardiac_Pct, "%\n")
cat("Phosphorylation research:", focus_analysis$Phosphorylation_Pct, "%\n") 
cat("Arrhythmia research:", focus_analysis$Arrhythmia_Pct, "%\n")
cat("Signaling research:", focus_analysis$Signaling_Pct, "%\n")
```

## Knowledge Gaps Identification

```{r knowledge-gaps}
# Identify potential knowledge gaps
gap_analysis <- combined_metadata %>%
  mutate(
    # Check for specific research areas in titles
    HasClinicalTrial = str_detect(tolower(Title), "clinical trial|randomized|rct"),
    HasReview = str_detect(tolower(Title), "review|meta-analysis"),
    HasMetaAnalysis = str_detect(tolower(Title), "meta.analysis|systematic review"),
    HasTherapeutic = str_detect(tolower(Title), 
                               "therapeutic|treatment|drug|inhibitor|therapy"),
    HasBiomarker = str_detect(tolower(Title), 
                             "biomarker|marker|diagnostic"),
    HasTranslational = str_detect(tolower(Title),
                                 "translational|clinical")
  ) %>%
  summarise(
    Total = n(),
    Clinical_Trials = sum(HasClinicalTrial, na.rm = TRUE),
    Reviews = sum(HasReview, na.rm = TRUE),
    Meta_Analyses = sum(HasMetaAnalysis, na.rm = TRUE),
    Therapeutic_Studies = sum(HasTherapeutic, na.rm = TRUE),
    Biomarker_Studies = sum(HasBiomarker, na.rm = TRUE),
    Translational_Studies = sum(HasTranslational, na.rm = TRUE)
  ) %>%
  mutate(
    Clinical_Trials_Pct = round(Clinical_Trials / Total * 100, 1),
    Reviews_Pct = round(Reviews / Total * 100, 1),
    Meta_Analyses_Pct = round(Meta_Analyses / Total * 100, 1),
    Therapeutic_Pct = round(Therapeutic_Studies / Total * 100, 1),
    Biomarker_Pct = round(Biomarker_Studies / Total * 100, 1),
    Translational_Pct = round(Translational_Studies / Total * 100, 1)
  )

cat("Knowledge Gap Analysis:\n")
cat("Clinical trials:", gap_analysis$Clinical_Trials_Pct, "% of literature\n")
cat("Meta-analyses:", gap_analysis$Meta_Analyses_Pct, "% of literature\n")
cat("Therapeutic studies:", gap_analysis$Therapeutic_Pct, "% of literature\n")
cat("Biomarker studies:", gap_analysis$Biomarker_Pct, "% of literature\n")
cat("Translational studies:", gap_analysis$Translational_Pct, "% of literature\n")

# Visualize gaps
gap_viz_data <- data.frame(
  Category = c("Clinical Trials", "Meta-Analyses", "Therapeutic", 
               "Biomarker", "Translational"),
  Percentage = c(gap_analysis$Clinical_Trials_Pct, gap_analysis$Meta_Analyses_Pct,
                gap_analysis$Therapeutic_Pct, gap_analysis$Biomarker_Pct,
                gap_analysis$Translational_Pct)
)

gap_plot <- ggplot(gap_viz_data, aes(x = reorder(Category, Percentage), y = Percentage)) +
  geom_col(fill = "coral", alpha = 0.8) +
  coord_flip() +
  labs(
    title = "Research Gap Analysis - Clinical Translation Focus",
    x = "Research Category",
    y = "Percentage of Literature (%)",
    caption = "Lower percentages indicate potential research gaps"
  ) +
  theme_minimal()

print(gap_plot)

# Export high-quality figure
ggsave(filename = "figures/knowledge_gaps.png", 
       plot = gap_plot, 
       width = 12, height = 8, dpi = 300)
cat("✅ Figure saved: figures/knowledge_gaps.png\n")
```

---

# Data Export and Validation

## Export Results

```{r data-export}
# Create comprehensive export with clickable PubMed links
# Add hyperlinks to Literature Database
literature_db_with_links <- combined_metadata %>%
  mutate(
    # Add clickable PubMed link column
    PubMed_Link = paste0('=HYPERLINK("https://pubmed.ncbi.nlm.nih.gov/', 
                         PMID, '/", "View Study")')
  ) %>%
  # Reorder columns to put link after PMID
  select(PMID, PubMed_Link, Title, Authors, Journal, Year, everything())

export_data <- list(
  "Literature_Database" = literature_db_with_links,
  "Publication_Timeline" = timeline_data,
  "Journal_Distribution" = journal_data,
  "Author_Frequency" = data.frame(Author = names(author_freq), 
                                 Publications = as.numeric(unname(author_freq))),
  "Term_Frequency" = data.frame(Term = names(term_freq), 
                               Frequency = as.numeric(unname(term_freq))),
  "Research_Trends" = trend_analysis,
  "Gap_Analysis" = gap_viz_data
)

# Export to Excel
export_file <- paste0("results/CAMK2D_Literature_Analysis_", Sys.Date(), ".xlsx")
write.xlsx(export_data, export_file, overwrite = TRUE)

cat("Data exported to:", export_file, "\n")

# Create summary report
summary_stats <- list(
  total_publications = nrow(combined_metadata),
  year_range = paste(min(combined_metadata$Year, na.rm = TRUE), 
                    max(combined_metadata$Year, na.rm = TRUE), sep = "-"),
  unique_journals = length(unique(combined_metadata$Journal)),
  unique_authors = length(unique(unlist(lapply(author_data$AuthorList, function(x) x)))),
  search_strategies = length(search_queries),
  analysis_date = Sys.Date()
)

cat("\n=== Literature Mining Summary ===\n")
cat("Total publications analyzed:", summary_stats$total_publications, "\n")
cat("Year range:", summary_stats$year_range, "\n")
cat("Unique journals:", summary_stats$unique_journals, "\n")
cat("Unique authors:", summary_stats$unique_authors, "\n")
cat("Search strategies used:", summary_stats$search_strategies, "\n")
cat("Analysis completed:", summary_stats$analysis_date, "\n")
```

## Validation and Quality Checks

```{r validation}
# Validate data quality and completeness
validation_results <- list(
  data_completeness = quality_metrics,
  temporal_coverage = nrow(timeline_data),
  content_richness = length(term_freq),
  network_connectivity = ifelse(exists("network_graph"), 
                               vcount(network_graph), 0),
  export_success = file.exists(export_file)
)

cat("=== Validation Results ===\n")
cat("Data completeness check: PASSED\n")
cat("Temporal coverage:", validation_results$temporal_coverage, "years\n")
cat("Content analysis terms:", validation_results$content_richness, "\n")
cat("Author network nodes:", validation_results$network_connectivity, "\n")
cat("Export file created:", validation_results$export_success, "\n")

# Final data preview
cat("\n=== Final Dataset Preview ===\n")

# Create table with clickable PubMed links
preview_data <- combined_metadata %>%
  select(PMID, Title, Journal, Year, Authors) %>%
  head(10) %>%
  mutate(
    # Create clickable title links to PubMed
    Title = paste0('<a href="https://pubmed.ncbi.nlm.nih.gov/', PMID, 
                   '/" target="_blank">', Title, '</a>')
  )

DT::datatable(
  preview_data,
  caption = "Interactive view of literature database (click titles to view studies on PubMed)",
  options = list(scrollX = TRUE, pageLength = 5),
  escape = FALSE  # Allow HTML content for clickable links
)
```

---

# Technical Appendices

## Session Information

```{r session-info}
# Document session information for reproducibility
cat("=== Session Information ===\n")
sessionInfo()

# Package versions
cat("\n=== Key Package Versions ===\n")
key_packages <- c("RISmed", "tm", "topicmodels", "igraph", "tidyverse")
for (pkg in key_packages) {
  if (require(pkg, character.only = TRUE)) {
    cat(pkg, ":", as.character(packageVersion(pkg)), "\n")
  }
}

# System information
cat("\n=== System Information ===\n")
cat("R version:", R.version.string, "\n")
cat("Platform:", R.version$platform, "\n")
cat("Analysis date:", Sys.Date(), "\n")
cat("Analysis time:", Sys.time(), "\n")
```

## Troubleshooting Guide

### Common Issues and Solutions

1. **PubMed API Connection Issues**
   - Check internet connection
   - Verify RISmed package installation
   - Consider using alternative rentrez package
   - Implement retry logic for failed queries

2. **Memory Issues with Large Datasets**
   - Reduce maximum results per query
   - Process data in batches
   - Use sparse matrix representations
   - Clear unused objects with `rm()` and `gc()`

3. **Text Mining Performance**
   - Optimize corpus preprocessing
   - Use parallel processing for large datasets
   - Consider sampling for exploratory analysis
   - Cache intermediate results

4. **Network Analysis Limitations**
   - Filter to active authors only
   - Adjust network density thresholds
   - Use alternative layout algorithms
   - Consider weighted networks

---

# Conclusions and Next Steps

## Key Findings

1. **Publication Trends**: CAMK2D research shows steady growth with peaks in recent years
2. **Research Focus**: Strong emphasis on cardiac applications and phosphorylation mechanisms
3. **Author Networks**: Collaborative research clusters identified among leading researchers
4. **Knowledge Gaps**: Limited clinical trials and translational studies

## Recommendations for Future Research

1. Increase clinical trial investigations
2. Develop more therapeutic applications
3. Expand biomarker research
4. Enhance translational studies

## Integration with Other Analyses

This literature mining analysis provides the foundation for:
- GEO database transcriptomic analysis (02_geo_transcriptomics.Rmd)
- Phosphoproteomic network analysis (03_phosphoproteomics.Rmd)
- Integrated meta-analysis (04_meta_analysis.Rmd)

The identified knowledge gaps and research trends inform the focus areas for subsequent analyses in the CAMK2D research automation pipeline.

---

*Analysis completed: `r Sys.time()`*

*Report generated using CAMK2D Research Automation System*